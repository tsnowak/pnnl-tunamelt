# PNNL-TUNAMELT

## Pacific Northwest National Lab's - Tracking Underwater Nautical Activity around Marine Energy LocaTions dataset

<p align="center">
    <img src="media/2010-09-09_054500_HF_S010-original.gif" height="400" />
    <img src="media/2010-09-09_054500_HF_S010-detected.gif" height="400" />
</p>

This code accompanies the paper [PNNL-TUNAMELT: Toward automating the detection of interactions with marine energy devices using acoustic camera sensors](https://aslopubs.onlinelibrary.wiley.com/doi/10.1002/lom3.70024) which proposes a novel, labeled data set and automated detection pipeline for detecting fish in acoustic camera video around operating tidal turbines.

### Citing this Work
```
@article{nowak_lom_2025,
    author = {Nowak, Theodore and Staines, Garrett and Abdullai, Blerim},
    title = {PNNL-TUNAMELT: Toward automating the detection of interactions with marine energy devices using acoustic camera sensors},
    journal = {Limnology and Oceanography: Methods},
    year = {2026},
    volume = {n/a},
    number = {n/a},
    pages = {e70024},
    doi = {https://doi.org/10.1002/lom3.70024},
    url = {https://aslopubs.onlinelibrary.wiley.com/doi/abs/10.1002/lom3.70024},
    eprint = {https://aslopubs.onlinelibrary.wiley.com/doi/pdf/10.1002/lom3.70024},
    abstract = {Abstract Acoustic cameras, or imaging sonars, are often used to monitor marine energy sites in regions where the water is too dark or turbid for optical sensing. To do so more effectively, scientists are investigating automated detection methodologies to use on these data. However, prior work has found that existing automated detection approaches struggle with the dynamic image background around marine energy devices—such as moving turbine blades. While open-access datasets, methods, and standard evaluation metrics are needed to quickly develop and compare novel automated detection methods, none yet exist for this domain. Using previously collected data, in this work we created a labeled dataset of possible marine life interactions in acoustic camera video around an operating tidal turbine. We call this dataset the Pacific Northwest National Laboratory dataset for Tracking Underwater Nautical Activity around Marine Energy LocaTions or PNNL TUNAMELT dataset. In addition to this dataset, we developed an automated detection pipeline which filters noise from the acoustic camera imagery and then performs object detection to identify possible targets. To analyze our automated detection pipeline, we used a series of common detection and classification metrics. In doing so, we found that our pipeline detected 98\% of targets and removed 70\% of target-less frames in our dataset. These results illustrate our method's potential utility as an aid to a human analyst tasked with extracting targets of interest from the dataset. Finally, we openly release our labeled dataset and all associated code to support and encourage future work in this domain.}
}
```

### :fish: Disclaimer :fish:

This entire dataset was manually reviewed 3 times by fish biologists to ensure label quality and to identify possible fish collision or injury. To the best of our knowledge no fish were harmed in the making of this data set. Just keep swimmin' :tropical_fish:

# Getting Started

### Create a virtual environment

```bash
python3 -m venv env
source env/bin/activate
```

### Install dependencies

```bash
pip install -e .
```

### Download the data set

Download and unpack the dataset into `pnnl-tunamelt/data`

```bash
python scripts/download_dataset.py
```

The unpacked dataset structure is:
``` bash
PNNL-TUNAMELT
├── labels
│   └── cvat-video-1.1
│       ├── batched_test
│       ├── default
│       ├── test
│       └── train
└── mp4
    ├── batched_test
    ├── default
    ├── test
    └── train
```
Inside directories under `labels` are `.xml` files generated by [CVAT](https://github.com/cvat-ai/cvat) which contain the per-frame bounding boxes. Inside the directories under `mp4` are `.mp4` files of the acoustic camera videos. Test set video files were batched into video files of up to 300 frames for ease of processing. 

### Other data locations:
- [Data.gov](https://catalog.data.gov/dataset/the-pnnl-tunamelt-dataset-for-automated-detection-around-marine-energy-devices)
- [MHKDR](https://mhkdr.openei.org/submissions/633)

## Examples

Executables used to run our method and visualize results are at `scripts/` and `scripts/experiments`. Analysis of experimental results were often done inside jupyters notebooks which are located at `notebooks/`. Some examples of what these scripts do are:

### Run then visualize TUNAMELT using parameter set 20 on a video 18

```bash
python scripts/experiments/demo_inference.py --params scripts/experiments/params/20.json --show True --id 18
```

### Rerun TUNAMELT on the entire training split of the data set

```bash
python scripts/experiments/multirun.py --params scripts/experiments/params/20.json --dataset train
```

### Visualize the results generated by TUNAMELT (you'll need to generate results first using run or multirun first!)

```bash
python scripts/experiments/view_result.py scripts/experiments/results
```

### Perform a hyperparameter search using the training split

```bash
python scripts/experiments/multirun.py --params scripts/experiments/params/hparam_search.json --datset train
```

### Rerun the ablation study

```bash
python scripts/experiments/ablation_tracklet.py
```

# More Information

## Expected Directory Structure

```bash
pnnl-tunamelt
├── data
├── env
├── LICENSE
├── MANIFEST.in
├── media
├── notebooks
├── pyproject.toml
├── README.md
├── scripts
└── src
```

## Creating symlinks for the data directories

The code as it's written relies on a certain directory structure:

```bash
# for methods execution and results generation
ln -s <path/to/my/data> pnnl-tunamelt/data
# for results analysis in notebooks
ln -s <path/to/my/data> pnnl-tunamelt/notebooks/data
```

The directory structure should then look like:

```bash
pnnl-tunamelt/data/PNNL-TUNAMELT
├── labels
└── mp4
```

Currently `cvat-video-1.1` is the only format of labels supported

## Data set label structure

During data set creation video labels are converted from `xml` files into python objects. Below is the structure of the python objects returned by the data loader:

```python
{'test': [
    {
        'filename': '2010-09-08_074500_HF_S002_S001.mp4',
        'tracks': [
            {
                'frames': [
                    {'box': ((486, 1011), (542, 1062)),
                    'frame': 16,
                    'keyframe': 1,
                    'occluded': 0,
                    'outside': 0},
                    {'box': ((455, 1016), (511, 1067)), ...
                ]
                'label': 'target',
                'track_id': 0
            },
                'frames': ...
        ],
        'video_id': 12,
        'video_length': 160,
        'video_shape': {'height': 1792, 'width': 1032}
    }
]
}
```

## About pCloud Data Downloads

### Manually downloading the data set

The TUNAMELT data set proposed in this work is hosted on pcloud. A [web interface link](https://u.pcloud.link/publink/show?code=k76italK) is provided. For those who want a direct download link in order to download the data in a headless environment (such as a terminal session), a few steps must be taken.

### Creating a direct download link with pcloud

We will first use pcloud's web API to generate a direct download link. To do so, we will take the `code` parameter in web interface link, and pass it into the pcloud web API as shown below. **This link will always remain the same and can be copy/pasted.**

```
https://api.pcloud.com/getpublinkdownload?code=k76italK&forcedownload=0
```

This will return a page like the below. **This return will change for each request, as the direct download link always expires in one day!**

```
{
	"result": 0,
	"expires": "Sun, 12 Jan 2025 00:18:11 +0000",
	"dwltag": "UJFEyYCyJkhOOariSp0I5B",
	"path": "\/cBZnt4E1dZHhHGlcZZZ09k2XkZTFZZGiFZkZnR6fMJZR4ZoQZWQZ28ZJYZk8Z3HZVFZqQZJ8ZbLZBYZTRZmQZtd8tVZmvXij5CMNlhKk6hSqB8iU8gyhCLV\/PNNL-TUNAMELT.tar.gz",
	"hosts": [
		"p-def4.pcloud.com",
		"vc1044.pcloud.com"
	]
}
```

We can then choose a host to download from either `p-def4.pcloud.com` or `vc1044.pcloud.com` and append the `path` with backslashes removed -
`/cBZnt4E1dZHhHGlcZZZ09k2XkZTFZZGiFZkZnR6fMJZR4ZoQZWQZ28ZJYZk8Z3HZVFZqQZJ8ZbLZBYZTRZmQZtd8tVZmvXij5CMNlhKk6hSqB8iU8gyhCLV/PNNL-TUNAMELT.tar.gz` - to create the direct download link. The URL for this generated (and likely now expired) direct download path is given below. This can then be used with `wget` to download the data set in a terminal session.

```
https://p-def4.pcloud.com/cBZnt4E1dZHhHGlcZZZ09k2XkZTFZZGiFZkZnR6fMJZR4ZoQZWQZ28ZJYZk8Z3HZVFZqQZJ8ZbLZBYZTRZmQZtd8tVZmvXij5CMNlhKk6hSqB8iU8gyhCLV/PNNL-TUNAMELT.tar.gz
```

## About the Computer Vision Annotation Tool (CVAT)

[CVAT](https://github.com/cvat-ai/cvat/tree/develop) is an open-source image and video labeling tool that can be downloaded and stood up. We used CVAT's docker image to create a video labeling platform for our expert-annotators to identify targets of interest in video.

### Exporting Video Annotations from CVAT

Exporting all project annotations in CVAT does not preserve per-video frame information (at the time of writing). We therefore have to export each task manually. To do so using the CVAT CLI create a python environment [via the linked](https://github.com/cvat-ai/cvat/tree/develop/cvat-cli) then use the below to export annotations from a specific task (or just download annotations for each video by hand):

```bash
# format = CVAT for images 1.1
# task = 103
cli.py dump --format "CVAT for images 1.1" 103 output.zip
```




