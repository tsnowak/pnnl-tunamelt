
# Introduction

This code accompanies the paper `TBD` which studies possible methods for filtering turbine motion out of underwater acoustic camera videos in order to assist with fish detection and underwater turbine monitoring.

We operate on `.mp4` files transcoded from `.ddfs` generated by a DIDSON acoustic camera due to their greater ease of use and the plentiful tools available for doing so.

While our work focuses on improving filtering around underewater turbines, the motivation for this work is to enhance regulators' ability to assess fish interaction with water power devices. We therefore evaluate the efficacy of our filters by implementing them in a greater system used to determine when a potential fish target is in view. We then measure the accuracy of our determinations against expert-labeled, ground-truth data denoting when a target is in view. This target-in-frame detection system uses an out-of-the-box contour detection method (from OpenCV) on-top of our filtering methods to determine when a target is in frame.

Data used in this work can be found `TBD`.

# Getting Started

## Downloading conda

All package dependencies for this code base are handled by `conda` and `PyPi`. To use this repository, these managers will need to be installed:

- [Miniconda](https://docs.conda.io/en/latest/miniconda.html): Miniconda setup script downloads. Will also contain pip/PyPi.

## Conda environment setup

Once `conda` is setup on your machine, you can go ahead and setup the environment.

``` bash
# creates a conda environment for this code
conda create -n turbx python=3.9

# activates the new environment
conda activate turbx

# installs the software for this project into the environment
conda env update --file env.yml
```

## Data directory symlink

While not necessary, the tests in the `test` and the jupyter noteboks in the `notebooks` directory rely on data being stored in a `data` directory at the project level and notebooks level respectively. To do so I created a symlink to where data is stored on my machine.

``` bash
ln -s <path/to/my/data> ./data
ln -s <path/to/my/data> ./notebooks/data
```

# Data Structure and Labels

## Data Sets and Data Loaders: An Introduction and Example

The notion of a data set and data loader are used to pre-process and present data to our filter methods.

For smaller projects and individual pieces of data, a user might just write unique code to load each piece of data. When dealing with large datasets and for code that is meant to be shared however, it is valuable to write code that can load and preprocess many pieces of data without user intervention. Data sets are used to load, bundle, and organize data into splits. Then a data loader is used to present batches of data to processing methods.

An example implementation of our data set and data loader is given below. A *data set* is created from the videos and labels located at `path/to/mp4s` and `/path/to/labels`. This basically searches these directories for `mp4` and `xml` files respectively, loads the data into python data structures, and aligns video and label files. The *data loader* is then created from a *split* of this *data set* and returns an iterable object on which we can call the `next` method to get video-label pairs for processing. We can continue calling `next` until the data loader returns `StopIteration` which signals that we've returned all the data in our data set.

``` python
dataloader = DataLoader(
                Dataset(
                    videos='path/to/mp4s',
                    labels='path/to/labels'
                ),
                split="train",
)

# contains aligned video, label pairs
vid, label = next(dataloader)
```

## Video Label Data Structure

During data set creation video labels are converted from `xml` files into python objects. Below is the structure of the python objects returned by the data loader:

``` python
{'test': [
    {
        'filename': '2010-09-08_074500_HF_S002_S001.mp4',
        'tracks': [
            {
                'frames': [
                    {'box': ((486, 1011), (542, 1062)),
                    'frame': 16,
                    'keyframe': 1,
                    'occluded': 0,
                    'outside': 0},
                    {'box': ((455, 1016), (511, 1067)), ...
                ]
                'label': 'target',
                'track_id': 0
            },
                'frames': ...
        ],
        'video_id': 12,
        'video_length': 160,
        'video_shape': {'height': 1792, 'width': 1032}
    }
]
} 
```

# Running Tests

``` bash
# All tests
pytest -s

# Data-related tests
pytest -s test/data

# Dataloader-specific tests
pytest -s test/data/test_dataloader.py

# A specific suite of dataloader tests
pytest -s test/data/test_dataloader.py -k test_label

# With durations
pytest --durations=0 /test/data

# With profiling
python -m cProfile -m pytest --durations=0 /test/data
```

# CVAT

CVAT is an open-source image and video labeling tool that can be downloaded and stood up. We used CVAT's docker image to create a video labeling platform for our expert-annotators to identify targets of interest in video.

## Exporting Vdieo Annotations from CVAT

Exporting all project annotations does not preserve per-video frame information. We therefore have to export each task manually. To do so using the CVAT CLI create a python environment [via the linked](https://openvinotoolkit.github.io/cvat/docs/manual/advanced/cli/#usage) then use the below to export annotations from a specific task (or just download annotations for each video by hand):

``` bash
# format = CVAT for images 1.1
# task = 103
cli.py dump --format "CVAT for images 1.1" 103 output.zip
```
